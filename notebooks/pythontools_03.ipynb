{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hOyRXuMChdV"
      },
      "source": [
        "# Reconocimiento de audio y conversión a texto con Whisper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHt0MtxNChdY"
      },
      "source": [
        "## Importación de Bibliotecas\n",
        "\n",
        "Los siguientes comandos instalarán los paquetes de Python necesarios para grabar fragmentos de audio y utilizar los modelos Whisper para la transcripción de voz a texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR5G0oq0ChdZ",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install sounddevice wavio\n",
        "! pip install ipywebrtc notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7GZnH_JChda"
      },
      "source": [
        "También ocuparemos lo siguiente para poder grabar audio desde este cuaderno y poder procesar los archivos resultantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBoRlRLOChda",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!apt install ffmpeg\n",
        "!apt-get install libportaudio2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AT7T0kwChdb",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "from IPython.display import Audio, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q7-Y6OZChdb"
      },
      "source": [
        "## Realizamos una grabación de prueba\n",
        "\n",
        "Activaremos algunos widgets que nos proporciona Colab para poder realizar la grabación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HuxtdsQChdc",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmPsaBkyChdc"
      },
      "source": [
        "Pulsa el botón circular y empieza a hablar. Puede que no lo parezca, pero el widget estará capturando sonido. Pulsa de nuevo el botón del círculo cuando hayas terminado. El widget empezará inmediatamente a reproducir lo que ha capturado.\n",
        "\n",
        "Puedes usar la siguiente frase de Albert Einstein si no se te ocurre algo que grabar: \"Una persona que nunca ha cometido un error nunca intenta nada nuevo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neNaGKtlChdd",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)\n",
        "recorder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWIfo-_nChdd"
      },
      "source": [
        "El formato de audio capturado anteriormente no es legible por PyTorch. En este paso, convertimos nuestra grabación a un formato que PyTorch pueda entender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfP5xhXtChdd",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "with open('grabacion.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg -i grabacion.webm -ac 1 -f wav mi_grabacion.wav -y -hide_banner -loglevel panic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUrGdqp5Chdd"
      },
      "source": [
        "## Opciones de selección\n",
        "\n",
        "Whisper es capaz de realizar transcripciones para muchos idiomas (aunque funciona mejor para algunos idiomas que para otros).\n",
        "\n",
        "Whisper también es capaz de detectar el idioma de entrada. Sin embargo, para estar seguros, podemos decirle explícitamente a Whisper qué idioma debe esperar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoJ6KVxhChde",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "language_options = whisper.tokenizer.TO_LANGUAGE_CODE\n",
        "language_list = list(language_options.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LcZVyrkChde",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "lang_dropdown = widgets.Dropdown(options=language_list, value='spanish')\n",
        "output = widgets.Output()\n",
        "display(lang_dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb7VgbOcChde"
      },
      "source": [
        "Whisper también es capaz de realizar varias tareas, como la transcripción sólo en inglés, la traducción de cualquier idioma al inglés y la transcripción de en idioma original.\n",
        "\n",
        "A continuación puede seleccionar «transcripción» (que producirá texto en el mismo idioma que el de entrada) o «traducción» (que transcribirá de un idioma diferente al inglés a inglés).\n",
        "\n",
        "![Capacidades de Whisper](https://cdn.openai.com/whisper/draft-20220920a/asr-training-data-desktop.svg)\n",
        "\n",
        "Imagen tomada de [Introducción a Whisper](https://openai.com/blog/whisper/) by OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q28e7cNXChde",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "task_dropdown = widgets.Dropdown(options=['transcribe', 'translate'], value='transcribe')\n",
        "output = widgets.Output()\n",
        "display(task_dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKIxQGpNIuB7"
      },
      "source": [
        "## Cargamos el modelo Whisper\n",
        "\n",
        "Whisper viene en cinco tamaños de modelo, cuatro de los cuales también tienen una versión optimizada sólo en inglés. Este cuaderno carga modelos de tamaño «base» (más grandes que «tiny» pero más pequeños que los demás), que requieren aproximadamente 1 GB de RAM.\n",
        "\n",
        "Si ha seleccionado inglés arriba, la celda de abajo cargará la versión optimizada sólo en inglés. En caso contrario, cargará el modelo multilingüe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE4k2E6IJF2q"
      },
      "outputs": [],
      "source": [
        "if lang_dropdown.value == \"english\":\n",
        "  model = whisper.load_model(\"base.en\")\n",
        "else:\n",
        "  model = whisper.load_model(\"base\")\n",
        "print(\n",
        "    f\"El modelo es {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"y tiene {sum(np.prod(p.shape) for p in model.parameters()):,} parámetros.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6poh0JzJUUJ"
      },
      "source": [
        "Por último, establezcamos el resto de nuestras opciones de tarea e idioma a continuación y veamos lo que tenemos. Comprueba que las opciones de tarea e idioma son correctas, pero no te preocupes por el resto de valores predeterminados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZY8fZsFJc2C"
      },
      "outputs": [],
      "source": [
        "options = whisper.DecodingOptions(language=lang_dropdown.value, task=task_dropdown.value, without_timestamps=True)\n",
        "options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76sqpxfQJyxK"
      },
      "source": [
        "## Poniendo Whisper a prueba\n",
        "\n",
        "Todo lo que queda por hacer ahora es introducir nuestro audio en Whisper.\n",
        "La celda de abajo realiza los últimos pasos de procesamiento para que esto suceda. Primero, carga nuestro archivo de audio listo para PyTorch. A continuación, divide el audio en segmentos de 30 segundos. Crea un espectrograma log-mel del audio y lo introduce en Whisper junto con las opciones que establecimos anteriormente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1znbErenKDjK"
      },
      "outputs": [],
      "source": [
        "audio = whisper.load_audio(\"mi_grabacion.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "result = model.decode(mel, options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhtZBfI1KIoo"
      },
      "outputs": [],
      "source": [
        "result.text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
